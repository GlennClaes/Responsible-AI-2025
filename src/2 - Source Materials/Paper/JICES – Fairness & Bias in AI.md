## Source Material 
2025-10-28 13:05

Status: Draft 
Tags: [[Source Material]] [[3 - Tags/Paper|Paper]] [[Fairness]] [[Bias]] [[Sociotechnical Framework]]

# Fairness and bias in AI: a sociotechnical perspective

**Author(s):** Sanae El Mimouni
**Journal / Conference:** Journal of Information, Communication and Ethics in Society (JICES)
**Year:** 2025 (Published August 8)
**DOI / URL:** https://doi.org/10.1108/JICES-12-2024-0182 / https://www.emerald.com/jices/article/doi/10.1108/JICES-12-2024-0182/1271477/Fairness-and-bias-in-AI-a-sociotechnical

## Abstract

This paper aims to advance a comprehensive sociotechnical framework for addressing fairness and bias in AI systems, recognizing that purely technical solutions are insufficient. It targets AI deployment in sectors like hiring, lending, and criminal justice. The study proposes a framework combining algorithmic techniques, human oversight, regulatory frameworks, and stakeholder engagement.

## Key Findings

- **Bias definition:** AI bias includes statistical (selection, measurement, algorithmic) and social bias, amplifying existing societal inequities.

- **Fairness definition:** Fairness involves conflicting mathematical (demographic parity, etc.) and socio-cultural dimensions (justice, etc.), requiring value judgments.

- **Proposed Framework:** Integrates technical debiasing, stakeholder engagement, human oversight, regulatory compliance, and continuous evaluation.

- **Core Conclusion:** Combining technical expertise, social science insights, and diverse stakeholder perspectives is key to effective bias mitigation.

- **Limitations:** Purely technical solutions are insufficient due to simplified assumptions, trade-offs (e.g., accuracy), and lack of diverse perspectives.

- **Sociotechnical grounding:** The framework uses principles like joint optimization, contextual embeddedness, human agency, systemic view, and participatory design.

## Methodology Notes

- The study uses a **systematic analytical approach**.

- It combines a **structured literature review** (sociotechnical theory, AI fairness, governance frameworks from 2015-2024), **comparative framework analysis** (technical, social, governance approaches), and **case studies** (healthcare, criminal justice, hiring).

- The study acknowledges its **theoretical nature** and calls for future empirical validation.

## References

- Abid, A., Farooqi, M. and Zou, J. (2021), “Persistent anti-Muslim bias in large language models”, *arXiv preprint* arXiv:2101.05783.
- AI Global Governance Institute (2023), “AI Global Governance Commission”, available at: [AI Global Governance Commission](https://www.aiggcommission.com/) (Link assumed).
- Angwin, J., Larson, J., Mattu, S. and Kirchner, L. (2016), “Machine bias”, *ProPublica*, available at: [https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing).
- Barocas, S. and Selbst, A.D. (2016), “Big data’s disparate impact”, *California Law Review*, Vol. 104 No. 3, pp. 671-732.
- Baxter, G. and Sommerville, I. (2011), “Sociotechnical systems: from design methods to systems engineering”, *Interacting with Computers*, Vol. 23 No. 1, pp. 4-17.
- Benjamin, R. (2019), “Race after technology: abolitionist tools for the new Jim Code”, *Social Forces*, Vol. 98 No. 4, pp. 1-3.
- Bianchi, F., et al. (2023), “Easily accessible text-to-image generation amplifies demographic stereotypes at large scale”, *Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency*, pp. 58-76.
- Buolamwini, J. and Gebru, T. (2018), “Gender shades...”, *Proceedings of the 1st Conference on Fairness, Accountability and Transparency*, PMLR, Vol. 81, pp. 77-91.
- Caliskan, C.A., Bryson, J.J. and Narayanan, A. (2017), “Semantics derived automatically from language corpora contain human-like biases”, *Science*, Vol. 356 No. 6334, pp. 183-186.
- Chouldechova, A. (2017), “Fair prediction with disparate impact...”, *Big Data*, Vol. 5 No. 2, pp. 153-163.
- Corbett-Davies, S. and Goel, S. (2018), “The measure and mismeasure of fairness...”, *arXiv preprint* arXiv:1808.00023.
- Corbett-Davies, S., et al. (2017), “Algorithmic decision making and the cost of fairness”, *Proceedings of the 23rd ACM SIGKDD...*, pp. 797-806.
- Cowgill, B., et al. (2020), “Biased programmers? or biased data?...”, *Proceedings of the 21st ACM Conference on Economics and Computation*, pp. 679-681.
- Crawford, K. (2017), “The trouble with bias”, *Keynote Presentation at NeurIPS 2017*.
- Crawford, K. and Calo, R. (2016), “There is a blind spot in AI research”, *Nature*, Vol. 538 No. 7625, pp. 311-313.
- Datta, A., Tschantz, M.C. and Datta, A. (2014), “Automated experiments on ad privacy settings”, *Proceedings on Privacy Enhancing Technologies*, Vol. 2015 No. 1, pp. 92-112.
- Dignum, V. (2019), *Responsible Artificial Intelligence...*, Springer.
- Dignum, V. (2023), “Responsible artificial intelligence: recommendations...”, *Ethics and Information Technology*, Vol. 25 No. 1, pp. 1-12.
- Dobbe, R. and Wolters, A. (2024), “Toward sociotechnical AI...”, *Minds and Machines*, Vol. 34 No. 1, pp. 107-132.
- Emery, F.E. and Trist, E.L. (1965), “The causal texture of organizational environments”, *Human Relations*, Vol. 18 No. 1, pp. 21-32.
- Equal Credit Opportunity Act (ECOA) (1974).
- Eubanks, V.E. (2018), *Automating Inequality...*, St. Martin’s Press.
- European Commission (2019), “Ethics guidelines for trustworthy AI”.
- Fair Housing Act, 42 U.S.C. §3601 et seq (1968).
- Fjeld, J., et al. (2020), “Principled artificial intelligence...”, *SSRN Electronic Journal*.
- Ganguli, D., et al. (2022), “Red teaming language models...”, *arXiv preprint* arXiv:2209.07858.
- General Data Protection Regulation (GDPR) (2016).
- Green, B. and Hu, L. (2018), “The myth in the methodology...”, *ICML Workshop on Fairness, Accountability, and Transparency in Machine Learning*.
- Hardt, M., Price, E. and Srebro, N. (2016), “Equality of opportunity...”, *Advances in Neural Information Processing Systems*, Vol. 29, pp. 3315-3323.
- Hutchinson, B. and Mitchell, M. (2019), “50 Years of test (un)fairness...”, *Proceedings of the Conference on Fairness, Accountability, and Transparency*, pp. 49-58.
- Jasanoff, S. (2004), *States of Knowledge...*, Routledge.
- Jo, E.S. and Gebru, T. (2020), “Lessons from archives...”, *Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency*, pp. 306-316.
- John-Mathews, J.M., Cardon, D. and Balague, C. (2022), “From reality to world...”, *Journal of Business Ethics*, Vol. 178 No. 4, pp. 1127-1143.
- Kamiran, F. and Calders, T. (2012), “Data preprocessing techniques...”, *Knowledge and Information Systems*, Vol. 33 No. 1, pp. 1-33.
- Kleinberg, J., Mullainathan, S. and Raghavan, M. (2017), “Inherent trade-offs...”, *8th Innovations in Theoretical Computer Science Conference (ITCS 2017)*, Vol. 67, pp. 43:1-43:23.
- Kling, R. (2000), “Learning about information technologies...”, *The Information Society*, Vol. 16 No. 3, pp. 217-232.
- Kudina, O. and van de Poel, I. (2024), “A sociotechnical system perspective on AI”, *Minds and Machines*, Vol. 34 No. 3, pp. 11-35.
- Lai, V. and Tan, C. (2019), “On human predictions with explanations...”, *Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* 2019)*, pp. 29-38.
- Madaio, M.A., et al. (2020), “Co-designing checklists...”, *Proceedings of the 2020 CHI Conference...*, pp. 1-14.
- Mehrabi, N., et al. (2021), “A survey on bias and fairness...”, *ACM Computing Surveys*, Vol. 54 No. 6, pp. 1-35.
- Mumford, E. (1983), *Designing Human Systems...*, Manchester Business School.
- Obermeyer, Z., et al. (2019), “Dissecting racial bias...”, *Science*, Vol. 366 No. 6464, pp. 447-453.
- Oduro, S. and Kneese, T. (2024), “AI governance needs sociotechnical expertise”, *Data and Society*.
- Organisation for Economic Co-operation and Development (OECD) (2019), “OECD principles on artificial intelligence”.
- Partnership on AI (2019), “Responsible AI design assistant”.
- Passi, S. and Barocas, S. (2019), “Problem formulation and fairness”, *Proceedings of the Conference on Fairness, Accountability, and Transparency*, pp. 39-48.
- Raghavan, M., et al. (2020), “Mitigating bias in algorithmic hiring...”, *Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency*, pp. 469-481.
- Rahwan, I. (2017), “Society-in-the-loop...”, *Ethics and Information Technology*, Vol. 20 No. 1, pp. 5-14.
- Raji, I.D. and Buolamwini, J. (2022), “Actionable auditing revisited...”, *Communications of the ACM*, Vol. 66 No. 1, pp. 101-108.
- Richardson, R. (Ed.) (2021), *Confronting Black Boxes...*, AI Now Institute.
- Sartori, L. and Theodorou, A. (2022), “A sociotechnical perspective...”, *Ethics and Information Technology*, Vol. 24 No. 1, pp. 1-11.
- Selbst, A.D., et al. (2019), “Fairness and abstraction...”, *Proceedings of the Conference on Fairness, Accountability, and Transparency*, pp. 59-68.
- Srivastava, M., Heidari, H., and Krause, A. (2019), “Mathematical notions vs. human perception...”, *Proceedings of the 25th ACM SIGKDD...*, pp. 2459-2468.
- Suresh, H., and Guttag, J. (2021), “A framework for understanding sources of harm...”, *Proceedings of the 1st ACM Conference on Equity and Access...*, pp. 1-9.
- Trist, E.L. and Bamforth, K.W. (1951), “Some social and psychological consequences...”, *Human Relations*, Vol. 4 No. 1, pp. 3-38.
- West, S.M., Whittaker, M., and Crawford, K. (2019), *Discriminating Systems...*, AI Now Institute.
- Whittaker, M., et al. (2019), “Disability, bias and AI report”, *AI Now Institute*.